{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98c0e68-5d94-4682-8757-36f65e65453c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "# Azure Storage credentials\n",
    "storage_account_name = \"macavstorage\"\n",
    "container_name = \"datalake\"\n",
    "servicePrincipalID = \"0b27e0ea-e184-49cd-b921-b3519cb03f7f\"\n",
    "blobsecret = dbutils.secrets.get(scope=\"Scope1\", key=\"blobsecret1\")\n",
    "tenantID = \"60feac79-e042-4ce8-8759-dca313146110\"\n",
    "\n",
    "# Path to the file in Azure Data Lake\n",
    "# Create secret scope at \n",
    "# https://adb-4383697834848777.17.azuredatabricks.net/#secrets/createScope\n",
    "#Scope Name = Scope1\n",
    "#DNS Name = \"https://twitchkv.vault.azure.net/\" (Vault URI)\n",
    "#Resource ID = \"/subscriptions/972ad05f-b62e-48ab-a9fa-a17fd4dc6640/resourceGroups/twitchData/providers/Microsoft.KeyVault/vaults/twitchkv\" (Keyvault resource ID)\n",
    "\n",
    "#Initializing spark-session and adding configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeAzureStorage\") \\\n",
    "    .config(\"spark.sql.extensions\", \"delta.sql.DeltaSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Authenticating Serviceprincipal to access blob storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", \n",
    "               servicePrincipalID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", \n",
    "               blobsecret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", \n",
    "               f\"https://login.microsoftonline.com/{tenantID}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187a8f94-84f2-4b1e-bd2b-2a3d7c2377a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "\n",
    "# Set up Bearer Token (Replace with your actual token)\n",
    "\n",
    "BEARER_TOKEN = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken\")\n",
    "# Define the hashtag you want to search for (without #)\n",
    "HASHTAG = \"macavai\"\n",
    "\n",
    "# Twitter API endpoint for recent tweets with a hashtag\n",
    "url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "# Define query parameters\n",
    "params = {\n",
    "    \"query\": f\"#{HASHTAG} -is:reply\",  # Exclude replies\n",
    "    \"tweet.fields\": \"id,text,created_at,public_metrics,attachments,lang,source,author_id,entities\",\n",
    "    \"expansions\": \"attachments.media_keys,author_id\",\n",
    "    \"media.fields\": \"media_key,type,url\",\n",
    "    \"user.fields\": \"id,username\",\n",
    "    \"max_results\": 10,  # Adjust as needed (max 100)\n",
    "}\n",
    "\n",
    "# Set up headers with Bearer Token\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Make API request\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "tweets_data = response.json()\n",
    "print(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5712f1bf-09a0-4217-bde4-1162b60b8f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "# Extract data\n",
    "tweet = tweets_data['data'][0]\n",
    "user = tweets_data['includes']['users'][0]\n",
    "\n",
    "tweet_id = tweet['id']\n",
    "text = tweet['text']\n",
    "hashtags = \",\".join([tag['tag'] for tag in tweet.get('entities', {}).get('hashtags', [])])\n",
    "author_id = tweet['author_id']\n",
    "geo = tweet.get(\"geo\", {}).get(\"place_id\", \"N/A\")\n",
    "username = user['username']\n",
    "created_at = tweet['created_at']\n",
    "\n",
    "# Public metrics\n",
    "metrics = tweet['public_metrics']\n",
    "retweet_count = metrics['retweet_count']\n",
    "reply_count = metrics['reply_count']\n",
    "like_count = metrics['like_count']\n",
    "quote_count = metrics['quote_count']\n",
    "bookmark_count = metrics['bookmark_count']\n",
    "impression_count = metrics['impression_count']\n",
    "\n",
    "# Get API completed timestamp (current time in UTC)\n",
    "#api_completed_timestamp = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "# Sample data\n",
    "tweet_data = {\n",
    "    \"id\": tweet_id,\n",
    "    \"text\": text,\n",
    "    \"hashtags\": hashtags,\n",
    "    \"author_id\": author_id,\n",
    "    \"username\": username,\n",
    "    \"created_at\": created_at,\n",
    "    \"retweet_count\": retweet_count,\n",
    "    \"reply_count\": reply_count,\n",
    "    \"like_count\": like_count,\n",
    "    \"quote_count\": quote_count,\n",
    "    \"bookmark_count\": bookmark_count,\n",
    "    \"impression_count\": impression_count,\n",
    "    \"geo_place_id\": geo,\n",
    "    \"api_completed_timestamp\": 0\n",
    "}\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame([Row(**tweet_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73559896-e401-4223-9be9-5e991e50604d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/datalake/x_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0cf7e4-73de-4ca7-9139-f0efecfb2172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://datalake@macavstorage.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ea45e4-fb70-4187-aae7-b8183505f3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get X API Data and write to DataLake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
