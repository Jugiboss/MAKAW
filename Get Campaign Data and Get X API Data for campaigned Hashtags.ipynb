{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98c0e68-5d94-4682-8757-36f65e65453c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "# Azure Storage credentials\n",
    "storage_account_name = \"macavstorage\"\n",
    "container_name = \"datalake\"\n",
    "servicePrincipalID = \"0b27e0ea-e184-49cd-b921-b3519cb03f7f\"\n",
    "blobsecret = dbutils.secrets.get(scope=\"Scope1\", key=\"blobsecret1\")\n",
    "tenantID = \"60feac79-e042-4ce8-8759-dca313146110\"\n",
    "\n",
    "# Path to the file in Azure Data Lake\n",
    "# Create secret scope at \n",
    "# https://adb-4383697834848777.17.azuredatabricks.net/#secrets/createScope\n",
    "#Scope Name = Scope1\n",
    "#DNS Name = \"https://twitchkv.vault.azure.net/\" (Vault URI)\n",
    "#Resource ID = \"/subscriptions/972ad05f-b62e-48ab-a9fa-a17fd4dc6640/resourceGroups/twitchData/providers/Microsoft.KeyVault/vaults/twitchkv\" (Keyvault resource ID)\n",
    "\n",
    "#Initializing spark-session and adding configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeAzureStorage\") \\\n",
    "    .config(\"spark.sql.extensions\", \"delta.sql.DeltaSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Authenticating Serviceprincipal to access blob storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", \n",
    "               servicePrincipalID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", \n",
    "               blobsecret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", \n",
    "               f\"https://login.microsoftonline.com/{tenantID}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a3e4-f284-441f-96dc-ff95fa09c29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "campaigns_fetched = spark.read.format(\"delta\").load(f\"abfs://{container_name}@{storage_account_name}.dfs.core.windows.net/datalake/campaigns\")\n",
    "display(campaigns_fetched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a013bb6f-5849-4aa9-9e6b-c42ffb1e4768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in campaigns_fetched.select(\"hashtags\").collect():\n",
    "    for hashtag in row[\"hashtags\"]:\n",
    "        print(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "961f99f1-3bcf-4b12-bfcd-0770462273cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to fetch user details by author_id\n",
    "def fetch_user_by_author_id(author_id, token):\n",
    "    url = f\"https://api.twitter.com/2/users/{author_id}\"\n",
    "    \n",
    "    # Headers with Bearer Token\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    "\n",
    "    # API request\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise error if request fails\n",
    "        \n",
    "        user_data = response.json()\n",
    "        \n",
    "        # Extract username and other details if they exist\n",
    "        if \"data\" in user_data:\n",
    "            user = user_data[\"data\"]\n",
    "            return {\n",
    "                \"id\": user[\"id\"],\n",
    "                \"username\": user[\"username\"],\n",
    "                \"name\": user.get(\"name\", \"N/A\")\n",
    "            }\n",
    "        else:\n",
    "            print(f\"User with author_id {author_id} not found.\")\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch user for author_id {author_id}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c1f33a-2a7e-471e-8359-5b1c658a6b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ChatGPT 03/04/2025 - THIS WORKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56edd67b-036a-48f4-a409-3d86d7d77a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "# Toggle between tokens\n",
    "token_switch = True\n",
    "\n",
    "# Set up Bearer Tokens (Replace with your actual tokens)\n",
    "BEARER_TOKEN1 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken\")\n",
    "BEARER_TOKEN2 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken2\")\n",
    "\n",
    "\n",
    "# Loop through all tweets in all_tweets_data\n",
    "all_tweet_data = []\n",
    "\n",
    "# Iterate through all tweets\n",
    "for tweet in all_tweets_data:\n",
    "    author_id = tweet['author_id']\n",
    "\n",
    "    # Alternate between tokens\n",
    "    token = BEARER_TOKEN1 if token_switch else BEARER_TOKEN2\n",
    "    token_switch = not token_switch  # Flip token for next request\n",
    "\n",
    "    # Fetch user details using the author_id\n",
    "    user = fetch_user_by_author_id(author_id, token)\n",
    "\n",
    "    if user:\n",
    "        tweet_id = tweet['id']\n",
    "        text = tweet['text']\n",
    "        hashtags = \",\".join([tag['tag'] for tag in tweet.get('entities', {}).get('hashtags', [])])\n",
    "        geo = tweet.get(\"geo\", {}).get(\"place_id\", \"N/A\")\n",
    "        username = user['username']  # Extracted from user lookup\n",
    "        created_at = tweet['created_at']\n",
    "\n",
    "        # Public metrics\n",
    "        metrics = tweet['public_metrics']\n",
    "        retweet_count = metrics['retweet_count']\n",
    "        reply_count = metrics['reply_count']\n",
    "        like_count = metrics['like_count']\n",
    "        quote_count = metrics['quote_count']\n",
    "        bookmark_count = metrics['bookmark_count']\n",
    "        impression_count = metrics['impression_count']\n",
    "\n",
    "        # API completed timestamp (current time in UTC)\n",
    "        api_completed_timestamp = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "        # Prepare data for DataFrame\n",
    "        tweet_data = {\n",
    "            \"id\": tweet_id,\n",
    "            \"text\": text,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"author_id\": author_id,\n",
    "            \"username\": username,\n",
    "            \"created_at\": created_at,\n",
    "            \"retweet_count\": retweet_count,\n",
    "            \"reply_count\": reply_count,\n",
    "            \"like_count\": like_count,\n",
    "            \"quote_count\": quote_count,\n",
    "            \"bookmark_count\": bookmark_count,\n",
    "            \"impression_count\": impression_count,\n",
    "            \"geo_place_id\": geo,\n",
    "            \"api_completed_timestamp\": api_completed_timestamp\n",
    "        }\n",
    "\n",
    "        all_tweet_data.append(tweet_data)\n",
    "\n",
    "    # Rate limiting handling\n",
    "    time.sleep(1)  # Adjust as needed to avoid hitting rate limits\n",
    "\n",
    "# Convert the collected tweet data to a Spark DataFrame\n",
    "twitter_data = spark.createDataFrame([Row(**tweet) for tweet in all_tweet_data])\n",
    "\n",
    "\n",
    "# Writing the Spark DataFrame to Azure Data Lake as Parquet format\n",
    "#spark_df.write.parquet(data_lake_path, mode=\"overwrite\")\n",
    "display(spark_df)\n",
    "# Optionally, if you want to write in JSON format, use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b4bb4749-edd6-4efc-91d3-f893b0c4f359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10c2629-c714-44b1-8b4f-c35543c5e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Own code Added bearer token2 03/04/2025, doesn't return user.fields with second api key - USE THE ABOVE INSTEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187a8f94-84f2-4b1e-bd2b-2a3d7c2377a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DONT USE!!!\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "# Set up Bearer Tokens (Replace with your actual tokens)\n",
    "BEARER_TOKEN1 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken\")\n",
    "BEARER_TOKEN2 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken2\")\n",
    "\n",
    "all_tweets_data = []\n",
    "\n",
    "# Toggle between tokens\n",
    "token_switch = True\n",
    "\n",
    "# Loop through hashtags in campaigns_fetched\n",
    "for row in campaigns_fetched.select(\"hashtags\").collect():\n",
    "    for hashtag in row[\"hashtags\"]:\n",
    "        HASHTAG = hashtag\n",
    "\n",
    "        # Alternate between tokens\n",
    "        token = BEARER_TOKEN1 if token_switch else BEARER_TOKEN2\n",
    "        token_switch = not token_switch  # Flip token for next request\n",
    "\n",
    "        # Twitter API endpoint\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "        # Query parameters\n",
    "        params = {\n",
    "            \"query\": f\"#{HASHTAG} -is:reply\",  # Exclude replies\n",
    "            \"tweet.fields\": \"id,text,created_at,public_metrics,attachments,lang,source,author_id,entities\",\n",
    "            \"expansions\": \"attachments.media_keys,author_id\",\n",
    "            \"media.fields\": \"media_key,type,url\",\n",
    "            \"user.fields\": \"id,username\",\n",
    "            \"max_results\": 10,  # Adjust as needed\n",
    "        }\n",
    "\n",
    "        # Headers with Bearer Token\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "\n",
    "        # API request with error handling\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()  # Raise error if request fails\n",
    "            tweets_data = response.json()\n",
    "\n",
    "            # Append only if data exists\n",
    "            if \"data\" in tweets_data:\n",
    "                all_tweets_data.extend(tweets_data[\"data\"])  # Add new tweets without overwriting\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch tweets for #{HASHTAG}: {e}\")\n",
    "\n",
    "        time.sleep(10)  # Prevent rate limiting\n",
    "\n",
    "# Print collected tweets\n",
    "print(all_tweets_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25391511-3e1d-49b4-b388-1c8004907092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5712f1bf-09a0-4217-bde4-1162b60b8f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#OLD DO NOT USE\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "# Extract data\n",
    "tweet = all_tweets_data[0]\n",
    "user = all_tweets_data['includes']['users'][0]\n",
    "\n",
    "tweet_id = tweet['id']\n",
    "text = tweet['text']\n",
    "hashtags = \",\".join([tag['tag'] for tag in tweet.get('entities', {}).get('hashtags', [])])\n",
    "author_id = tweet['author_id']\n",
    "geo = tweet.get(\"geo\", {}).get(\"place_id\", \"N/A\")\n",
    "username = user['username']\n",
    "created_at = tweet['created_at']\n",
    "\n",
    "# Public metrics\n",
    "metrics = tweet['public_metrics']\n",
    "retweet_count = metrics['retweet_count']\n",
    "reply_count = metrics['reply_count']\n",
    "like_count = metrics['like_count']\n",
    "quote_count = metrics['quote_count']\n",
    "bookmark_count = metrics['bookmark_count']\n",
    "impression_count = metrics['impression_count']\n",
    "\n",
    "# Get API completed timestamp (current time in UTC)\n",
    "#api_completed_timestamp = datetime.datetime.utcnow().isoformat()\n",
    "\n",
    "# Sample data\n",
    "tweet_data = {\n",
    "    \"id\": tweet_id,\n",
    "    \"text\": text,\n",
    "    \"hashtags\": hashtags,\n",
    "    \"author_id\": author_id,\n",
    "    \"username\": username,\n",
    "    \"created_at\": created_at,\n",
    "    \"retweet_count\": retweet_count,\n",
    "    \"reply_count\": reply_count,\n",
    "    \"like_count\": like_count,\n",
    "    \"quote_count\": quote_count,\n",
    "    \"bookmark_count\": bookmark_count,\n",
    "    \"impression_count\": impression_count,\n",
    "    \"geo_place_id\": geo,\n",
    "    \"api_completed_timestamp\": 0\n",
    "}\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame([Row(**tweet_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73559896-e401-4223-9be9-5e991e50604d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/datalake/datalake/x_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0cf7e4-73de-4ca7-9139-f0efecfb2172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"abfss://datalake@macavstorage.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ea45e4-fb70-4187-aae7-b8183505f3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xdata_fetched = spark.read.format(\"delta\").load(f\"abfs://{container_name}@{storage_account_name}.dfs.core.windows.net/datalake/datalake/x_data\")\n",
    "\n",
    "display(xdata_fetched)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get Campaign Data and Get X API Data for campaigned Hashtags",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
