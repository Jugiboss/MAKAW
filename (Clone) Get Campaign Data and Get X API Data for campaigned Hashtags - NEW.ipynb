{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98c0e68-5d94-4682-8757-36f65e65453c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "# Azure Storage credentials\n",
    "storage_account_name = \"macavstorage\"\n",
    "container_name = \"datalake\"\n",
    "servicePrincipalID = \"0b27e0ea-e184-49cd-b921-b3519cb03f7f\" \n",
    "blobsecret = dbutils.secrets.get(scope=\"Scope1\", key=\"blobsecret1\")\n",
    "tenantID = \"60feac79-e042-4ce8-8759-dca313146110\"\n",
    "file = 'x_data'\n",
    "\n",
    "# Path to the file in Azure Data Lake\n",
    "# Create secret scope at \n",
    "# https://adb-4383697834848777.17.azuredatabricks.net/#secrets/createScope\n",
    "#Scope Name = Scope1\n",
    "#DNS Name = \"https://twitchkv.vault.azure.net/\" (Vault URI)\n",
    "#Resource ID = \"/subscriptions/972ad05f-b62e-48ab-a9fa-a17fd4dc6640/resourceGroups/twitchData/providers/Microsoft.KeyVault/vaults/twitchkv\" (Keyvault resource ID)\n",
    "\n",
    "#Initializing spark-session and adding configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeAzureStorage\") \\\n",
    "    .config(\"spark.sql.extensions\", \"delta.sql.DeltaSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Authenticating Serviceprincipal to access blob storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", \n",
    "               servicePrincipalID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", \n",
    "               blobsecret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", \n",
    "               f\"https://login.microsoftonline.com/{tenantID}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a3e4-f284-441f-96dc-ff95fa09c29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "campaigns_fetched = spark.read.format(\"delta\").load(f\"abfs://{container_name}@{storage_account_name}.dfs.core.windows.net/datalake/campaigns\")\n",
    "display(campaigns_fetched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a013bb6f-5849-4aa9-9e6b-c42ffb1e4768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in campaigns_fetched.select(\"hashtags\").collect():\n",
    "    for hashtag in row[\"hashtags\"]:\n",
    "        print(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9d0bc9-e946-4c87-9a2a-abb1893b363c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Set up Bearer Tokens\n",
    "BEARER_TOKEN1 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken\")\n",
    "BEARER_TOKEN2 = dbutils.secrets.get(scope=\"Scope1\", key=\"XBearerToken2\")\n",
    "\n",
    "# Token toggle\n",
    "token_switch = True\n",
    "\n",
    "# Twitter API endpoint\n",
    "url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "all_tweet_data = []\n",
    "\n",
    "# Loop through hashtags from campaigns_fetched\n",
    "for row in campaigns_fetched.select(\"hashtags\").collect():\n",
    "    for hashtag in row[\"hashtags\"]:\n",
    "        token = BEARER_TOKEN1 if token_switch else BEARER_TOKEN2\n",
    "        token_switch = not token_switch\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {token}\"\n",
    "        }\n",
    "\n",
    "        params = {\n",
    "            \"query\": f\"#{hashtag} -is:reply\",\n",
    "            \"tweet.fields\": \"id,text,created_at,public_metrics,geo,entities,author_id\",\n",
    "            \"expansions\": \"author_id\",\n",
    "            \"user.fields\": \"id,username,name\",\n",
    "            \"max_results\": 100,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 429:\n",
    "                reset_time = int(response.headers.get(\"x-rate-limit-reset\", time.time() + 60))\n",
    "                sleep_for = max(0, reset_time - time.time()) + 1\n",
    "                print(f\"Rate limit hit. Sleeping for {sleep_for:.0f} seconds...\")\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "\n",
    "            users_map = {user[\"id\"]: user for user in result.get(\"includes\", {}).get(\"users\", [])}\n",
    "\n",
    "            for tweet in result.get(\"data\", []):\n",
    "                author_id = tweet[\"author_id\"]\n",
    "                user = users_map.get(author_id, {})\n",
    "\n",
    "                tweet_data = {\n",
    "                    \"id\": tweet[\"id\"],\n",
    "                    \"text\": tweet[\"text\"],\n",
    "                    \"hashtags\": \",\".join([tag['tag'] for tag in tweet.get(\"entities\", {}).get(\"hashtags\", [])]),\n",
    "                    \"author_id\": author_id,\n",
    "                    \"username\": user.get(\"username\", \"N/A\"),\n",
    "                    \"created_at\": tweet[\"created_at\"],\n",
    "                    \"retweet_count\": tweet[\"public_metrics\"].get(\"retweet_count\", 0),\n",
    "                    \"reply_count\": tweet[\"public_metrics\"].get(\"reply_count\", 0),\n",
    "                    \"like_count\": tweet[\"public_metrics\"].get(\"like_count\", 0),\n",
    "                    \"quote_count\": tweet[\"public_metrics\"].get(\"quote_count\", 0),\n",
    "                    \"bookmark_count\": tweet[\"public_metrics\"].get(\"bookmark_count\", 0),\n",
    "                    \"impression_count\": tweet[\"public_metrics\"].get(\"impression_count\", 0),\n",
    "                    \"geo_place_id\": tweet.get(\"geo\", {}).get(\"place_id\", \"N/A\"),\n",
    "                    \"api_completed_timestamp\": datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "                }\n",
    "\n",
    "                all_tweet_data.append(tweet_data)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch tweets for #{hashtag}: {e}\")\n",
    "\n",
    "        # Sleep to avoid hitting short-term burst rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "unique_tweets = {tweet[\"id\"]: tweet for tweet in all_tweet_data}\n",
    "deduped_tweet_data = list(unique_tweets.values())\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "twitter_data = spark.createDataFrame([Row(**tweet) for tweet in deduped_tweet_data])\n",
    "\n",
    "# Write to Data Lake (adjust path/mode if needed)\n",
    "# twitter_data.write.parquet(data_lake_path, mode=\"overwrite\")\n",
    "\n",
    "# Display in notebook\n",
    "display(twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c1f33a-2a7e-471e-8359-5b1c658a6b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ChatGPT 03/04/2025 - THIS WORKS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a10c2629-c714-44b1-8b4f-c35543c5e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Own code Added bearer token2 03/04/2025, doesn't return user.fields with second api key - USE THE ABOVE INSTEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a88665-4bf4-4333-a075-66c0bc272dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e214dde4-7b48-4960-8657-696b22c6774c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "twitter_data.write \\\n",
    "    .format(\"json\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74896e84-b3d2-443f-a308-81d89b86cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "read Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db54c02-d004-4d7e-add0-9e67345aede8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xdata_fetched_json = spark.read.format(\"json\").load(f\"abfs://{container_name}@{storage_account_name}.dfs.core.windows.net/x_data\")\n",
    "\n",
    "display(xdata_fetched_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Get Campaign Data and Get X API Data for campaigned Hashtags - NEW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
